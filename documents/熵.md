# 熵

## 比特化（Bits）

假设存在一组随机变量X，各个值出现的概率如图：

|值|A|B|C|D|
|---|---|---|---|---|
|概率|1/4|1/4|1/4|1/4|

现在有一组由X变量组成的序列：BACADDCBAC...；如果现在希望将这个序列转换为二进制来进行网络传输，我们可以使用两个比特位表示一个随机变量

|值|A|B|C|D|
|---|---|---|---|---|
|编码|00|01|10|11|

那么我们得到一个这样的序列：01001000111110010010

当X变量出现的概率值不一样的时候，对于一组序列信息来讲，每个变量平均需要多少个比特位来描述呢？

概率：

|值|A|B|C|D|
|---|---|---|---|---|
|概率|1/2|1/4|1/8|1/8|

编码：

|值|A|B|C|D|
|---|---|---|---|---|
|编码|0|10|110|111|

期望：

$$ E=1 \times \frac{1}{2} + 2 \times \frac{1}{4} + 3 \times \frac{1}{8} +  3 \times \frac{1}{8} = 1.75 $$

即：

$$ E=-\log_2(\frac{1}{2}) \times \frac{1}{2} - \log_2(\frac{1}{4}) \times \frac{1}{4} -\log_2(\frac{1}{8}) \times \frac{1}{8} -\log_2(\frac{1}{8}) \times \frac{1}{8} = 1.75 $$

## 一般化的比特化（Bits）
假设现在随机变量X具有m个值，分别为：$v_1,v_2,\cdots,v_m$，并且各个值出现的概率如下：

|值|$v_1$|$v_2$|$\cdots$|$v_m$|
|---|---|---|---|---|
|概率|$p_1$|$p_2$|$\cdots$|$p_m$|

每个变量平均需要多少个比特位描述呢？

$$
\begin{aligned}
E(X)&=-p_1\log_2(p_1)-p_2\log_2(p_2)-\cdots-p_m\log_2(p_m)\\
&=-\sum_{i=1}^mp_i\log_2(p_i)
\end{aligned}
$$

## 信息熵（Entropy）
$H(X)$就叫做随机变量的信息熵：

$$H(X)=-\sum_{i=1}^mp(x_i)\log_2p(x_i)$$

信息量指一个事件所蕴含的信息，如果一个事件概率越大，那么就可以认为该时间所蕴含的信息越少。极端情况下，比如：“太阳从东方升起”，因为是确定事件，所以不懈怠任何信息量。

信息熵就是衡量一个系统有序程度的度量，一个系统越有序，信息熵就越低，一个系统越混乱，信息熵就越高。

信息熵就是用来描述系统信息量的不确定度。

高信息熵（High Entropy）:表示随机变量X是均匀分布的，各种取值情况是等概率出现的。
低信息熵（Low Entropy）:表示随机变量X各种取值不是等概率出现。

## 联合熵
两个随机变量X和Y的联合分布可以形成联合熵，定义为联合自信息的数学期望，它是二维随机变量X，Y蛋不确定性的度量，用$H(X,Y)$表示：

$$
H(X,Y)=-\sum_{i=1}^n\sum_{j=1}^np(x_i,y_j)\log_2p(x_i,y_j)
$$

## 条件熵
随机事件(X,Y)发生所包含的熵，减去事件X单独发生的熵，即为在随机变量X发生的前提下，随机变量Y发生新带来的熵，定义为Y的条件熵，即用$H(Y|X)$表示：

$$
\begin{aligned}
H(Y|X)&=H(X,Y)-H(X)\\
&=-\sum_{x,y}p(x,y)\log_2p(y|x) \\

H(Y|X)&=\sum_xp(x) H(Y|x)
\end{aligned}
$$

$$
$$

推导：

$$
\begin{aligned}
&H(Y|X)=H(X,Y)-H(X)\\
&=-\sum_{x,y}p(x,y)\log p(x,y) + \sum_xp(x) \log p(x)\\
&=-\sum_{x,y}p(x,y)\log p(x,y) + \sum_x\left( \sum_yp(x,y) \right) \log p(x)\\
&=-\sum_{x,y}p(x,y)\log p(x,y) + \sum_{x,y}p(x,y) \log p(x)\\
&=-\sum_{x,y}p(x,y)\log \frac{p(x,y)}{p(x)}\\
&= \underline{ -\sum_{x,y}p(x,y)\log p(y|x)}\\
&=  -\sum_{x,y}p(x)p(y|x)\log p(y|x)\\
&= \sum_xp(x) \left( -\sum_yp(y|x) \log p(y|x) \right)\\
&= \underline{\sum_xp(x) H(Y|x)}
\end{aligned}
$$

## 互信息
两个随机变量$X$、$Y$ 的互信息:$X$、$Y$的联合分布和各自独立分布乘积的相对熵称为互 信息，用$I(X;Y)$表示。互信息是信息论里一种有用的信息度量方式，它可以看作一个随机变 量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变 量而减少的不肯定性。互信息是两个事件集合之间的相关性。

$$I(X;Y)=\sum_{x \in X}\sum_{y \in Y}p(x,y) \log \frac{p(x,y)}{p(x)p(y)}$$

互信息、熵和条件熵之间存在以下关系:

$$ H(Y|X)=H(Y)-I(X;Y)$$

推导过程：

$$
\begin{aligned}
& H(Y)-I(X;Y) \\
& = -\sum_y p(y) \log p(y)-\sum_{x,y}p(x,y) \log \frac{p(x,y)}{p(x)p(y) } \\
& = -\sum_y \left( \sum_xp(x,y)\right) \log p(y)-\sum_{x,y}p(x,y) \log \frac{p(x,y)}{p(x)p(y) } \\
& = -\sum_{x,y} p(x,y) \log p(y)-\sum_{x,y}p(x,y) \log \frac{p(x,y)}{p(x)p(y) } \\
& = -\sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)} \\
& = -\sum_{x,y} p(x,y) \log p(y|x) \\
& = H(Y|X)

\end{aligned}
$$

通过上面的计算过程发现$H(Y|X)=H(Y)-I(X,Y)$,又由前面条件熵的定义$H(Y|X)=H(X,Y)-H(X)$,于是有$I(X,Y)=H(X)+H(Y)-H(X,Y)$,此即为大多数文献中互信息的定义。

![互信息](/assets/images/熵/互信息.png)

实际上，互信息体现了两变量之间的依赖程度：如果$I(X,Y) \gg 0$，表明X和Y是高度相关的；如果$I(X,Y) = 0$表明X和Y是相互独立的；如果$I(X,Y) \ll 0$，表明Y的出现不但未使X的不确定性减小，反而增大了X的

## 相对熵（KL散度）
相对熵是衡量相同事件空间里两个概率分布相对差距的测度。两个概率分布$p(x)$和$q(x)$的相对熵定义为：

$$
D(p||q) = \sum_{x \in X}p(x) \log \frac{p(x)}{q(x)}
$$

注意：$D(P||Q) \neq D(Q||P)$

互信息实际上就是衡量一个联合分布于独立性差距多大的测度：

$$
I(X;Y) = D(p(x,y)||p(x)p(y))
$$

## 交叉熵

用来衡量估计模型与真实概率分布之间差异情况。如果一个随机变量$X \sim p(x)$，$q(x)$为用于近似$p(x)$的概率分布，那么，随机变量X和模型q之间的交叉熵定义为：

$$
\begin{aligned}
H(X,q) &= H(X) + D(p||q) \\
&=-\sum_xp(x) \log q(x)
\end{aligned}
$$

## 点互信息 PMI 

Pointwise Mutual Information,用来衡量两个事物之间的相关性（比如两个词）

$$
PMI(x;y) = \log \frac{p(x,y)}{p(x)p(y)} = \log \frac{p(x|y)}{p(x)} = \log \frac{p(y|x)}{p(y)}
$$

x,y不相关，$PMI(x;y)=0$,x,y相关$p(x,y)>p(x)p(y)$

互信息其实就是对X，Y所有可能取值情况的点互信息的加权和，即:


$$
\begin{aligned}
I(X;Y)&=\sum_{x \in X}\sum_{y \in Y}p(x,y) \log \frac{p(x,y)}{p(x)p(y)} \\
&=\sum_{x \in X}\sum_{y \in Y}p(x,y) PMI(x;y) 
\end{aligned}
$$

例子：
衡量like这个词的极性（正向情感还是负向情感），我们可以预先挑选一些正向情感的词，如good，然后计算like跟good的PMI。

$$
PMI(like,good) = \log \frac{p(like,good)}{p(like)p(good)}
$$



---
**参考**：
1. 宗成庆《统计自然语言处理》（第2版）
2. 唐聃 等《自然语言处理理论与实战》
3. [sklearn：点互信息和互信息](https://blog.csdn.net/u013710265/article/details/72848755)