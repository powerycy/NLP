# 分词

## 分词原理
分词方法如下：
基于词典的匹配:前向最大匹配、后向最大匹配。
基于字的标注:最大熵模型、条件随机场模型、感知器模型。 
其他方法:与词性标注结合、与句法分析结合。

## 分词工具

### 1. jieba

[github仓库](https://github.com/fxsjy/jieba)

支持三种分词模式：

* 精确模式，试图将句子最精确地切开，适合文本分析；
* 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；
* 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。

#### API
1. jieba.cut(sentence, cut_all=False, HMM=True)  
返回generator
* sentence:需要分词的字符串
* cut_all:参数用来控制是否采用全模式
* HMM:参数用来控制是否使用 HMM 模型(可发现新词)

2. jieba.cut_for_search(sentence, HMM=True)  
返回generator  
该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细
* sentence:需要分词的字符串
* HMM:参数用来控制是否使用 HMM 模型(可发现新词)


```python
import jieba

seg_list = jieba.cut("我来到北京清华大学", cut_all=True)
print("全模式: " + "/ ".join(seg_list))  # 全模式

seg_list = jieba.cut("我来到北京清华大学", cut_all=False)
print("精确模式（默认）: " + "/ ".join(seg_list))  # 精确模式

seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式
print("搜索引擎模式：" + "/ ".join(seg_list)) # 搜索引擎模式

```
结果：
```
全模式: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学
精确模式（默认）: 我/ 来到/ 北京/ 清华大学
搜索引擎模式：小明/ 硕士/ 毕业/ 于/ 中国/ 科学/ 学院/ 科学院/ 中国科学院/ 计算/ 计算所/ ，/ 后/ 在/ 日本/ 京都/ 大学/ 日本京都大学/ 深造
```


3. jieba.lcut  
同`jieba.cut`，但返回为list

4. jieba.lcut_for_search  
同`jieba.cut_for_search`，但返回为list

5. jieba.load_userdict(file_name)  
加载自定义词典
词典格式一词一行，一行三部分：词语、词频（可省略）、词性（可省略）空格隔开  
* file_name：为文件类对象或自定义词典的路径

```python

test_sent = "李小福是创新办主任也是云计算方面的专家; 什么是八一双鹿"
words = jieba.cut(test_sent)
print("加载词典前：" + '/'.join(words))

jieba.load_userdict("userdict.txt")

words = jieba.cut(test_sent)
print("加载词典后：" + '/'.join(words))
```
结果：
```
加载词典前：李小福/是/创新/办/主任/也/是/云/计算/方面/的/专家/;/ /什么/是/八/一双/鹿
加载词典后：李小福/是/创新办/主任/也/是/云计算/方面/的/专家/;/ /什么/是/八一双鹿
```

6. jieba.add_word(word, freq=None, tag=None)  
可在程序中动态修改词典

7. jieba.del_word(word)  
可在程序中动态修改词典

8. jieba.suggest_freq(segment, tune=True)  
可调节单个词语的词频，使其能（或不能）被分出来。

```python

print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))
jieba.suggest_freq(('中', '将'), True)
print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))

print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False)))
jieba.suggest_freq('台中', True)
print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False)))

```
结果：
```
如果/放到/post/中将/出错/。
如果/放到/post/中/将/出错/。
「/台中/」/正确/应该/不会/被/切开
「/台中/」/正确/应该/不会/被/切开
```

9. jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())  
关键词抽取  
* sentence: 待提取的文本
* topK: 返回几个 TF/IDF 权重最大的关键词，默认值为 20
* withWeight: 是否一并返回关键词权重值，默认值为 False
* allowPOS: 仅包括指定词性的词，默认值为空，即不筛选

10. jieba.analyse.TFIDF(idf_path=None)  
新建 TFIDF 实例
* idf_path: IDF 频率文件

11. jieba.analyse.set_idf_path(file_name)  
切换成自定义idf的路径
* file_name: 自定义idf的路径

```python
jieba.analyse.set_idf_path("../extra_dict/idf.txt.big")

tags = jieba.analyse.extract_tags(content, topK=topK)
```

12. jieba.analyse.set_stop_words(file_name)  
切换成自定义停止词的路径
* file_name: 自定义停止词的路径

```python

jieba.analyse.set_stop_words("../extra_dict/stop_words.txt")
jieba.analyse.set_idf_path("../extra_dict/idf.txt.big")

tags = jieba.analyse.extract_tags(content, topK=topK)
```

13. jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))  
基于 TextRank 算法的关键词抽取，接口相同，注意默认过滤词性。

14. jieba.analyse.TextRank()  
新建自定义 TextRank 实例

15. jieba.posseg.POSTokenizer(tokenizer=None)  
新建自定义分词器
* tokenizer： 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。
```python
import jieba.posseg as pseg
words = pseg.cut("我爱北京天安门")
for word, flag in words:
    print('%s %s' % (word, flag))
```
结果：
```
我 r
爱 v
北京 ns
天安门 ns
```

16. jieba.enable_parallel(processnum=None)  
开启并行分词模式  
将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升

* processnum： 并行进程数

17. jieba.disable_parallel() 
关闭并行分词模式

18. jieba.tokenize(unicode_sentence, mode="default", HMM=True)
返回词语在原文的起止位置  

```python

print("默认模式:")
result = jieba.tokenize('永和服装饰品有限公司')
for tk in result:
    print("word %s\t\t start: %d \t\t end:%d" % (tk[0],tk[1],tk[2]))

print("搜索模式:")
result = jieba.tokenize(u'永和服装饰品有限公司', mode='search')
for tk in result:
    print("word %s\t\t start: %d \t\t end:%d" % (tk[0],tk[1],tk[2]))

```
结果：

```
默认模式:
word 永和                start: 0                end:2
word 服装                start: 2                end:4
word 饰品                start: 4                end:6
word 有限公司            start: 6                end:10
搜索模式:
word 永和                start: 0                end:2
word 服装                start: 2                end:4
word 饰品                start: 4                end:6
word 有限                start: 6                end:8
word 公司                start: 8                end:10
word 有限公司            start: 6                end:10
```











