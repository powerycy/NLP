# 特征抽取

## 1. 词袋法（Bag of Words, BoW)
对文本分词后，统计每个次在文本中出现的次数，得到该文本基于词的特征，即向量化。向量化后一般会使用TF-IDF进行特征的权重修正，在进行标准化。因为该方法仅仅考虑了词频，没有考虑上下文关系，因此会丢失一部分文本语义，但如果我们的目的是分类聚类，则词袋模型大多数时候表现的很好

* 分词（tokenizing）
* 统计修订词特征值（counting）
* 标准化（normalizing）

例子：

```python
from sklearn.feature_extraction.text import CountVectorizer  
vectorizer=CountVectorizer()
corpus=["I come to China to travel", 
    "This is a car polupar in China",          
    "I love tea and Apple ",   
    "The work is to write some papers in science"] 
print(vectorizer.fit_transform(corpus))
```
```
  (0, 16)       1
  (0, 3)        1
  (0, 15)       2
  (0, 4)        1
  (1, 5)        1
  (1, 9)        1
  (1, 2)        1
  (1, 6)        1
  (1, 14)       1
  (1, 3)        1
  (2, 1)        1
  (2, 0)        1
  (2, 12)       1
  (2, 7)        1
  (3, 10)       1
  (3, 8)        1
  (3, 11)       1
  (3, 18)       1
  (3, 17)       1
  (3, 13)       1
  (3, 5)        1
  (3, 6)        1
  (3, 15)       1
```

输出结果左边括号中第一个数字是文本的序号，第2个数字是词的序号，注意词的序号是基于所有的文档的。第三个数字就是我们的词频。

```python

print(vectorizer.fit_transform(corpus).toarray())
print(vectorizer.get_feature_names())
```

```
[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]
 [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]
 [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]
 [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]]
['and', 'apple', 'car', 'china', 'come', 'in', 'is', 'love', 'papers', 'polupar', 'science', 'some', 'tea', 'the', 'this', 'to', 'travel', 'work', 'write']
```

可以看到我们一共有19个词，所以4个文本都是19维的特征向量。而每一维的向量依次对应了下面的19个词。另外由于词"I"在英文中是停用词，不参加词频的统计。

由于大部分的文本都只会使用词汇表中的很少一部分的词，因此我们的词向量中会有大量的0。也就是说词向量是稀疏的。在实际应用中一般使用稀疏矩阵来存储。

## 2. 词集法（Set of Words, Sow)  
与词袋模型唯一的不同是它仅仅考虑词是否在文本中出现过，而不考虑词频。

## 3. Hash Trick

在大规模的文本处理中，由于特征的维度对应分词词汇表的大小，所以维度可能非常恐怖，此时需要进行降维，不能直接用我们上一节的向量化方法。而最常用的文本降维方法是Hash Trick。说到Hash，一点也不神秘，学过数据结构的同学都知道。这里的Hash意义也类似。

在Hash Trick里，我们会定义一个特征Hash后对应的哈希表的大小，这个哈希表的维度会远远小于我们的词汇表的特征维度，因此可以看成是降维。具体的方法是，对应任意一个特征名，我们会用Hash函数找到对应哈希表的位置，然后将该特征名对应的词频统计值累加到该哈希表位置。如果用数学语言表示,假如哈希函数$h$使第$i$个特征哈希到位置$j$,即$h(i)=j$,则第$i$个原始特征的词频数值$\phi(i)$将累加到哈希后的第$j$个特征的词频数值$\bar{\phi}$上，即：

$$\bar{\phi}(j)=\sum_{i\in \mathcal {J};h(i)=j}\phi(i)$$

其中$\mathcal{J}$是原始特征的维度。

但是上面的方法有一个问题，有可能两个原始特征的哈希后位置在一起导致词频累加特征值突然变大，为了解决这个问题，出现了hash Trick的变种signed hash trick,此时除了哈希函数h,我们多了一个一个哈希函数：

$$\xi : \mathbb{N} \to {\pm 1}$$

此时我们有

$$\bar{\phi}(j) = \sum_{i\in \mathcal{J};h(i)=j}\xi(i)\phi(i)$$

这样做的好处是，哈希后的特征仍然是一个无偏的估计，不会导致某些哈希位置的值过大。

例子：
```python
from sklearn.feature_extraction.text import HashingVectorizer 
vectorizer2=HashingVectorizer(n_features = 6,norm = None)
print(vectorizer2.fit_transform(corpus))
```

```
  (0, 1)        2.0
  (0, 2)        -1.0
  (0, 4)        1.0
  (0, 5)        -1.0
  (1, 0)        1.0
  (1, 1)        1.0
  (1, 2)        -1.0
  (1, 5)        -1.0
  (2, 0)        2.0
  (2, 5)        -2.0
  (3, 0)        0.0
  (3, 1)        4.0
  (3, 2)        -1.0
  (3, 3)        1.0
  (3, 5)        -1.0
```

大家可以看到结果里面有负数，这是因为我们的哈希函数$\xi$可以哈希到1或者-1导致的。

## 4. TF-IDF
用词袋法，我们发现句子"I come to China to travel"中"come","China"和"travel"各出现一次，而"to"出现两次，虽然"to"出现次数更多，但"to"是一个非常普遍的词，在这句话里，我们知道他的重要性远不及"China"和"travel"。所以，仅仅采用词频无法反应这一点。

TF-IDF(Term Frequency -  Inverse Document Frequency)，即“词频-逆文档频率”则可以解决该问题。

词频TF越大表明这个词越重要，但如果一个词在很多文本中出现，那么他将变得不重要，比如上文中的"to"，他的IDF应该更低。反之，如果一个词在比较少的文本中出现，则表明这个词更有代表性，更重要，IDF应该更高

$$IDF(x) = log\frac{N}{N(x)}$$

其中，$N$代表语料库中文本的总数，而$N(x)$代表语料库中包含词x的文本总数。

如果某一个生僻词在语料库中没有，这样我们的分母为0， IDF没有意义了。所以常用的IDF我们需要做一些平滑

$$IDF(x) = log\frac{N+1}{N(x)+1} + 1$$

有了IDF的定义，我们就可以计算某一个词的TF-IDF值了：

$$TF-IDF(x) = TF(x) * IDF(x)$$

其中$TF(x)$指词x在当前文本中的词频。

例子：

方法一：

```python
# tf-idf 方法1

from sklearn.feature_extraction.text import TfidfTransformer  
from sklearn.feature_extraction.text import CountVectorizer  

corpus=["I come to China to travel", 
    "This is a car polupar in China",          
    "I love tea and Apple ",   
    "The work is to write some papers in science"] 

vectorizer=CountVectorizer()

transformer = TfidfTransformer()
tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))  
print(tfidf)
```

方法二：
```python
# tf-idf 方法2
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf2 = TfidfVectorizer()
re = tfidf2.fit_transform(corpus)
print(re)

```

由于第二种方法比较的简洁，因此在实际应用中推荐使用，一步到位完成向量化，TF-IDF与标准化。

---
**参考**：  
1. 刘建平博客：[文本挖掘预处理之向量化与Hash Trick](http://www.cnblogs.com/pinard/p/6688348.html)
2. 刘建平博客：[文本挖掘预处理之TF-IDF](https://www.cnblogs.com/pinard/p/6693230.html)
